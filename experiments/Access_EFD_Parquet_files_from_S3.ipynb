{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Parquet file from S3\n",
    "See https://kafka-connect-manager.lsst.io/ for more information on the S3 Sink connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"efd-sandbox.data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S3 credentials can be added to `~/.aws/credentials` file. They are stored in SQuaRE 1Password. Search for EFD AWS S3 credentials. The S3 region can be added to the`~/.aws/config` file.\n",
    "\n",
    "For example:\n",
    "```\n",
    "cat ~/.aws/credentials\n",
    "[default]\n",
    "aws_access_key_id = <the aws_access_key_id>\n",
    "aws_secret_access_key = <the aws_secret_access_key>\n",
    "```\n",
    "and\n",
    "```\n",
    "cat ~/.aws/config \n",
    "[default]\n",
    "region=us-east-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the S3 Sink connector is configured to partition data by time on an hourly basis. The following helps to construct the path to find the Parquet files on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"example-002-aggregated\"\n",
    "year = \"2020\"\n",
    "month = \"08\"\n",
    "day = \"07\"\n",
    "hour = \"22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `bucket.download_fileobj()` method to download the Parquet files into a buffer, and then Pyarrow to read the files and convert and append them to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for obj in bucket.objects.filter(Prefix=f\"topics/{topic}/year={year}/month={month}/day={day}/hour={hour}\"):\n",
    "    buffer = io.BytesIO()\n",
    "    bucket.download_fileobj(obj.key, buffer)\n",
    "    df = df.append(pq.read_table(buffer).to_pandas())\n",
    "    print(f\"{bucket.name}:{obj.key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S3 Sink connector is configured to invoke file commits to S3 every 10 minutes (see the `rotate_interval_ms` configuration setting) that's why you see 6 files in this path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the aggregated stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.plot(x='time', y='mean_value1', c='white', figsize=(15,5))\n",
    "p.fill_between(x='time', y1='min_value1', y2='max_value1', data=df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
