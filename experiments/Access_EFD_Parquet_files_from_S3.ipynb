{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Parquet file from S3\n",
    "See https://kafka-connect-manager.lsst.io/ for more information on the S3 Sink connector and references on it. For this example the S3 Sink connector is configure to partition data by time on an hourly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"efd-sandbox.data\"\n",
    "KEY = \"topics/example-002-aggregated/year=2020/month=08/day=06/hour=22/example-002-aggregated+0+0000022314.snappy.parquet\"\n",
    "#KEY = \"topics/example-000-aggregated/year=2020/month=08/day=06/hour=16/example-000-aggregated+0+0000000177.snappy.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 credentials are added to `~/.aws/credentials`file and the S3 region to the`~/.aws/config` file as explained here https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(BUCKET_NAME).download_file(KEY, 'example-002-aggregated+0+0000022314.snappy.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pyarrow read the Parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_002_aggregated = pq.read_table('example-002-aggregated+0+0000022314.snappy.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from Parquet to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = example_002_aggregated.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the aggregated stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.plot(x='time', y='mean_value1', c='white', figsize=(15,5))\n",
    "p.fill_between(x='time', y1='min_value1', y2='max_value1', data=df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
