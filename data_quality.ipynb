{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data quality flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the numerous data flags that are produced by the LSST stack processing. This is not meant to provide a recommendation about which flags to use, but is rather an illustration of how one can examine the effects of flags on a data set.\n",
    "\n",
    "(Based in part on notebooks originally written by Angelo Fausti and Sasha Brownsberger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSST stack imports\n",
    "from lsst.daf.persistence import Butler\n",
    "import lsst.afw.display as afw_display\n",
    "\n",
    "# Other python imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use results from coadds produced during the HSC weekly reprocessing. See more info here: https://confluence.lsstcorp.org/display/DM/S18+HSC+PDR1+reprocessing\n",
    "\n",
    "There is currently no convenient way to learn from the butler what dataset types are available, or what tracts, patches, and filters have data. So, we have to know in advance the name of the dataset type (e.g., `deepCoadd_forced_src`), the tract, patch IDs, and filter, which we can get from the wiki page linked above (see also [this summary of HSC-SSP data](https://hsc-release.mtk.nao.ac.jp/doc/index.php/database/)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### If you want to edit the data set, tract, and patch yourself, change the numbers below appropriately\n",
    "data_set = 'UDEEP'# HSC/SSP survey data include WIDE, DEEP, UDEEP fields\n",
    "datadir = '/datasets/hsc/repo/rerun/DM-13666/' + data_set \n",
    "butler = Butler(datadir)\n",
    "\n",
    "# We selected the \"Ultra-deep (UDEEP)\" data, and will choose a tract from the SXDS field (tract 8765):\n",
    "tract = 8765 #8766\n",
    "patch = '1,2' #'8,3'  # patch selected at random\n",
    "\n",
    "#All subsequent data structures will be stored in sets, with the elements of the sets \n",
    "# corresponding to the filters you specify here.  So, probably a good idea to remember the order! \n",
    "filters = ['HSC-G','HSC-I','HSC-Y'] \n",
    "n_filters = len(filters)\n",
    "\n",
    "# We'll focus our analysis on the forced photometry from the co-add data set:\n",
    "data_type = 'deepCoadd_forced_src'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the data for this patch using the butler, looping over the filters defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_coadds = [butler.get(data_type, tract = tract, patch = patch, dataId={'filter': filter}) for filter in filters]\n",
    "n_raw_objects = len(deep_coadds[0])\n",
    "for i in range(len(filters)):\n",
    "    print ('Number of objects in filter ' + filters[i] + ' = ' + str(len(deep_coadds[i])))\n",
    "print ('(Note that those numbers should all be the same.)')\n",
    "\n",
    "# Convert the catalogs to Astropy tables so we can easily work with them:\n",
    "deep_coadd_tables = [deep_coadd.asAstropy() for deep_coadd in deep_coadds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What columns are in the catalogs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = deep_coadd_tables[0]\n",
    "table.colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data flags in the catalogs:\n",
    "\n",
    "Here we filter all columns that have 'flag' in their names and get their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = [ (colname, table[colname].description) for colname in table.colnames if '_flag_' in colname]\n",
    "pd.DataFrame(flags, columns=['Column Name', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the fraction of objects rejected by each flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tables = [catalog.asAstropy() for catalog in catalogs]\n",
    "\n",
    "def compute_fraction(table, colname):\n",
    "    size = len(table)\n",
    "    fraction = int(len(table[table[colname]==True])/size*100)\n",
    "    return fraction\n",
    "\n",
    "fraction_rejected = [[colname] + [\"{}%\".format(compute_fraction(table, colname)) for table in deep_coadd_tables]\n",
    "                     for colname, description in flags] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a table of the fraction of objects rejected by each flag:\n",
    "\n",
    "df = pd.DataFrame(fraction_rejected, columns=['Flag'] + filters)\n",
    "\n",
    "# By default, Pandas only displays some rows at the beginning and end. This will force it to display all rows:\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there any spatial patterns in the flagged objects?\n",
    "\n",
    "Select a set of the flags to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_to_reject_on = ['base_PixelFlags_flag_bright_objectCenter',\n",
    "                      'base_PixelFlags_flag_saturated', \n",
    "                      'base_PixelFlags_flag_cr',\n",
    "                      'base_GaussianFlux_flag_badShape',\n",
    "                      'base_PixelFlags_flag_sensor_edge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spatial distribution of flagged objects in the first filter (in this case, 'HSC-G'):\n",
    "\n",
    "for flag in flags_to_reject_on:\n",
    "    index = deep_coadd_tables[0][flag]==True\n",
    "    plt.figure()\n",
    "# Note that (RA, Dec) are in radians -- use numpy.rad2deg() to convert to degrees.\n",
    "    plt.hist2d(numpy.rad2deg(deep_coadd_tables[0]['coord_ra'][index]), numpy.rad2deg(deep_coadd_tables[0]['coord_dec'][index]), bins=(100, 100), cmap='Blues')\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.xlabel('RA(deg)')\n",
    "    plt.ylabel('Dec(deg)')\n",
    "    plt.title(\"{} ({}%)\".format(flag, compute_fraction(deep_coadd_tables[0], flag)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all possible flags, and our recommended flags \n",
    "flags_to_reject_on_recommended= ['base_PixelFlags_flag_bad', 'base_PixelFlags_flag_cr', 'base_PixelFlags_flag_offimage','base_PixelFlags_flag_edge', 'base_PixelFlags_flag_saturated', 'base_PixelFlags_flag_rejected', 'base_PixelFlags_flag_interpolated']\n",
    "flags_to_reject_on_all = [ (colname, deep_coadd_tables[0][colname].description) for colname in deep_coadd_tables[0].colnames if '_flag_' in colname]\n",
    "#print (flags_to_reject_on_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Choose the flags on which you wish to reject observations. ')\n",
    "#print ('Here are your options: ')\n",
    "#print (flags_to_reject_on_all)\n",
    "#df_allflags = pd.DataFrame(flags_to_reject_on_all, columns=['Column Name', 'Description'])\n",
    "#with pd.option_context('display.max_rows', None):\n",
    "#    display(df_allflags)\n",
    "    \n",
    "print ('Here are our suggestions:')\n",
    "print (flags_to_reject_on_recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######ENTER YOUR CHOICE OF REJECTION FLAGS HERE##### \n",
    "flags_to_reject_on = [] #Leave empty to use \"recommended\" flags\n",
    "\n",
    "# EXAMPLE:\n",
    "#flags_to_reject_on = ['base_PixelFlags_flag_bright_objectCenter',\n",
    "#                      'base_PixelFlags_flag_saturated', \n",
    "#                      'base_PixelFlags_flag_cr',\n",
    "#                      'base_GaussianFlux_flag_badShape',\n",
    "#                      'base_PixelFlags_flag_sensor_edge']\n",
    "\n",
    "if len(flags_to_reject_on) == 0: \n",
    "    flags_to_reject_on = flags_to_reject_on_recommended[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do operation on all flags in individual filter, and then between filter.  \n",
    "# So if an object has ANY of the desired flags set, that object will be rejected by the master flag \n",
    "total_rej_flags_by_filters = [[any([deep_coadd_table[flag][i] for flag in flags_to_reject_on]) for i in range(n_raw_objects)] \n",
    "                                              for deep_coadd_table in deep_coadd_tables] \n",
    "\n",
    "for i in range(n_filters):\n",
    "    nok = len([flag for flag in total_rej_flags_by_filters[i] if flag == False])\n",
    "    print ('For filter ' + str(filters[i]) + ', ' + str(nok) +\\\n",
    "           ' of ' + str(n_raw_objects) + \" ({:.1%})\".format((nok/n_raw_objects)) + ' objects were \"good\" based on your chosen flags.')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More flags ###\n",
    "\n",
    "Those aren't the only data flags. There are also flags contained in the \"ref\" catalogs for each coadd data set. Many of these do not have \"flag\" in their name, but can be very important. This includes important fields such as \"detect_isPrimary\" and \"base_ClassificationExtendedness_value\", among others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'deepCoadd_ref'\n",
    "\n",
    "deep_coadd_refs = [butler.get(data_type, tract = tract, patch = patch, dataId={'filter': filter}) for filter in filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the catalogs to Astropy tables so we can easily work with them:\n",
    "deep_coadd_reftables = [deep_coadd_ref.asAstropy() for deep_coadd_ref in deep_coadd_refs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_table = deep_coadd_reftables[0]\n",
    "ref_table.colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.size(table.colnames))\n",
    "print(np.size(ref_table.colnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the ones with \"slot_\" in them, since they are simply repeats of the values contained elsewhere:\n",
    "rr_flags = [ (colname, ref_table[colname].description) for colname in ref_table.colnames if 'slot_' not in colname]\n",
    "pd.DataFrame(rr_flags[600:650], columns=['Column Name', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
